#!/usr/bin/env bash
# shellcheck disable=SC2126

set +e

# =============================================================================
# Globals
# =============================================================================
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
export SCRIPT_DIR
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
export PROJECT_DIR
OS="$(uname -s | tr '[:upper:]' '[:lower:]')"
export OS
export DEFAULT_PROXY="envoy"

declare -i DEFAULT_WORKERS=10
if [[ ${OS} == darwin ]]; then
  DEFAULT_WORKERS=$(sysctl -n hw.physicalcpu)
elif [[ ${OS} == linux ]]; then
  DEFAULT_WORKERS=$(nproc)
fi
export DEFAULT_WORKERS

if [[ $OS == darwin ]]; then
  BREW_HOME=$(brew --prefix)
fi

declare -a LOG_FILES=(
  "/var/log/nginx/access.log"
  "/var/log/nginx/error.log"
  "/var/log/envoy.log"
  "/var/log/haproxy.log"
  "/var/log/gomoku-httpd.log"
  "/var/log/gomoku-frontend.log"
)
export LOG_FILES

declare -a PACKAGES=(clang-format shfmt btop htop ctop bottom)
export PACKAGES

export HTOP_REGEX='|nginx|haproxy|envoy|gomoku-httpd|vite'

# Component order for stop/restart/status (reverse of start)
declare -a STOP_ORDER=(frontend gomoku envoy haproxy nginx)

# =============================================================================
# Process Management — self-contained, no bashmatic dependency
# =============================================================================

# Return PIDs matching $1, excluding lines matching $2.
# Uses bracket trick: pass "[n]ginx" so grep never matches itself.
# Always excludes monitoring tools (htop, btop, ctop, btm, gctl).
# Usage: cluster.pids "[e]nvoy"
#        cluster.pids "[g]omoku-httpd" "gomoku-http-client"
cluster.pids() {
  local pattern="$1"
  local exclude="${2:-}"
  local base_exclude="htop|btop|ctop|btm|gctl"
  if [[ -n "$exclude" ]]; then
    base_exclude="${base_exclude}|${exclude}"
  fi
  /bin/ps -eo pid,command |
    /usr/bin/grep -E "$pattern" |
    /usr/bin/grep -Ev "$base_exclude" |
    /usr/bin/awk '{print $1}' |
    /usr/bin/sort -n
}

# Return 0 if any PIDs match, 1 otherwise.
cluster.is-running() {
  [[ -n "$(cluster.pids "$@")" ]]
}

# Send TERM, wait, then KILL survivors. Optionally sudo.
cluster.kill() {
  local pattern="$1"
  local exclude="${2:-}"
  local use_sudo="${3:-}"
  local -a pids
  local pid

  mapfile -t pids < <(cluster.pids "$pattern" "$exclude")
  [[ ${#pids[@]} -eq 0 ]] && return 0

  for pid in "${pids[@]}"; do
    if [[ -n "$pid" ]]; then
      ${use_sudo} /bin/kill -TERM "$pid" 2>/dev/null || true
    fi
  done
  sleep 1

  mapfile -t pids < <(cluster.pids "$pattern" "$exclude")
  for pid in "${pids[@]}"; do
    if [[ -n "$pid" ]]; then
      ${use_sudo} /bin/kill -9 "$pid" 2>/dev/null || true
    fi
  done
}

# =============================================================================
# Setup
# =============================================================================

setup.bashmatic() {
  [[ -d ~/.bashmatic ]] || bash -c "$(curl -fsSL https://bashmatic.re1.re); bashmatic-install -q" >/dev/null 2>&1
  # shellcheck disable=SC1090
  source ~/.bashmatic/init >/dev/null 2>&1
}

setup.log-files-are-ok() {
  for file in "${LOG_FILES[@]}"; do
    [[ -w "$file" ]] || {
      error "Log file [${file}] either does not exist or is not writable."
      sleep 1
      hint "HINT: Run the following command first:" "$(basename "$0") setup"
      return 1
    }
  done
}

setup.sudo() {
  h3 "Setting up sudo and the ownership of the log files..."
  sudo -n date >/dev/null 2>&1 || {
    h.yellow "sudo is not authenticated, please enter your password to authenticate:"
    sudo -v || { error "sudo authentication failed."; return 1; }
  }

  run "sudo chmod 777 /var/log"
  for file in "${LOG_FILES[@]}"; do
    run "sudo mkdir -p $(dirname "$file")"
    run "sudo touch $file"
    run "sudo chown $USER $file"
    run "sudo chmod 666 $file"
  done
  run "sudo chown $USER /var/log/nginx"
  success "All log files exist and are writeable."
}

setup.direnv() {
  command -v direnv >/dev/null 2>&1 || package.install direnv
  command -v direnv >/dev/null 2>&1 && direnv allow .
}

setup.utils() {
  for pkg in "${PACKAGES[@]}"; do
    inf "Checking if $pkg is installed..."
    if package.is-installed "$pkg"; then
      ui.closer.ok:
    else
      ui.closer.kind-of-ok:
      inf "Installing $pkg..."
      package.install "$pkg" >/dev/null 2>&1 || true
      ui.closer.ok:
    fi
  done
}

cmd_setup() {
  setup.bashmatic
  setup.sudo
  setup.direnv
  setup.utils
}

# =============================================================================
# Config Generation
# =============================================================================

generate.configs() {
  local workers="${1:-10}"
  local proxy="${2:-${DEFAULT_PROXY}}"
  local template_dir="$PROJECT_DIR/iac/templates"

  h3 "Generating $proxy config for ${workers} workers..."
  if [[ "$proxy" == "haproxy" ]]; then
    local output="${BREW_HOME}/etc/haproxy.cfg"
    bash "$template_dir/haproxy.cfg.sh" "$workers" >"$output"
    success "Generated $output with $workers workers."
  else
    local output="$PROJECT_DIR/iac/envoy/envoy.yaml"
    bash "$template_dir/envoy.yaml.sh" "$workers" >"$output"
    success "Generated $output with $workers workers."
  fi
}

# =============================================================================
# Generic component stop / status — covers haproxy, envoy, frontend
# =============================================================================

# Generic stop: kill processes matching a grep pattern.
# Usage: component.stop <display-name> <grep-pattern> [exclude] [sudo]
component.stop() {
  local name="$1" pattern="$2" exclude="${3:-}" use_sudo="${4:-}"
  inf "Stopping $name..."

  if cluster.is-running "$pattern" "$exclude"; then
    cluster.kill "$pattern" "$exclude" "$use_sudo"
    sleep 1
    if cluster.is-running "$pattern" "$exclude"; then
      ui.closer.not-ok:
      error "$name failed to stop."
      return 1
    fi
    ui.closer.ok:
    success "$name stopped."
  else
    ui.closer.kind-of-ok:
    success "$name is not running."
  fi
}

# Generic status: show PIDs for a grep pattern.
component.status() {
  local name="$1" pattern="$2" exclude="${3:-}"
  inf "$name status..."

  if cluster.is-running "$pattern" "$exclude"; then
    ui.closer.ok:
    success "$name is running (PIDs: $(cluster.pids "$pattern" "$exclude" | tr '\n' ' '))"
  else
    ui.closer.kind-of-ok:
    warning "$name is not running."
  fi
}

# =============================================================================
# NGINX — requires sudo
# =============================================================================

nginx.start() {
  inf "Starting nginx..."

  if cluster.is-running "[n]ginx"; then
    ui.closer.kind-of-ok:
    success "nginx already running, $(cluster.pids '[n]ginx' | wc -l | tr -d ' ') processes."
    return 0
  fi

  sudo "${BREW_HOME}/bin/nginx" -g "daemon on;" 2>/dev/null
  sleep 2

  if cluster.is-running "[n]ginx"; then
    ui.closer.ok:
    success "nginx started."
  else
    ui.closer.not-ok:
    error "nginx failed to start — check /var/log/nginx/error.log"
    return 1
  fi
}

nginx.stop() {
  inf "Stopping nginx..."

  if ! cluster.is-running "[n]ginx"; then
    ui.closer.kind-of-ok:
    success "nginx is not running."
    return 0
  fi

  sudo "${BREW_HOME}/bin/nginx" -s stop 2>/dev/null || true
  sleep 2

  # Force-kill any survivors via xargs (handles root-owned processes)
  if cluster.is-running "[n]ginx"; then
    warning "nginx didn't stop gracefully, force killing..."
    cluster.pids "[n]ginx" | sudo xargs /bin/kill -9 2>/dev/null || true
    sleep 1
  fi

  if cluster.is-running "[n]ginx"; then
    ui.closer.not-ok:
    error "nginx failed to stop."
    return 1
  fi
  ui.closer.ok:
  success "nginx stopped."
}

nginx.restart() { nginx.stop; sleep 1; nginx.start; }
nginx.status() {
  component.status "nginx" "[n]ginx"
  if cluster.is-running "[n]ginx" && command -v curl >/dev/null 2>&1; then
    curl -s http://127.0.0.1/nginx_status 2>/dev/null || true
  fi
}

# =============================================================================
# HAPROXY
# =============================================================================

haproxy.start() {
  inf "Starting haproxy..."

  if cluster.is-running "[h]aproxy"; then
    ui.closer.kind-of-ok:
    success "haproxy is already running."
    return 0
  fi

  "${BREW_HOME}/opt/haproxy/bin/haproxy" -D -f "${BREW_HOME}/etc/haproxy.cfg"
  sleep 1

  if cluster.is-running "[h]aproxy"; then
    ui.closer.ok:
    success "haproxy started."
  else
    ui.closer.not-ok:
    error "haproxy failed to start."
    return 1
  fi
}

haproxy.stop() { component.stop "haproxy" "[h]aproxy"; }
haproxy.restart() { haproxy.stop; sleep 1; haproxy.start; }
haproxy.status() {
  component.status "haproxy" "[h]aproxy"
  if cluster.is-running "[h]aproxy" && [[ -f "${BREW_HOME}/etc/haproxy.cfg" ]]; then
    local url
    url=$(grep -A 3 stats "${BREW_HOME}/etc/haproxy.cfg" | grep bind | awk '{print $2}' | head -1)
    [[ -n "$url" ]] && echo "  Admin: http://${url}/stats"
  fi
}

# =============================================================================
# ENVOY
# =============================================================================

envoy.start() {
  inf "Checking if envoy is installed..."
  if ! command -v envoy >/dev/null 2>&1; then
    ui.closer.not-ok:
    inf "Installing envoy..."
    package.install envoy
    command -v envoy >/dev/null 2>&1 || { error "envoy is not installed."; return 1; }
    ui.closer.ok:
  else
    ui.closer.ok:
  fi

  local envoy_config="$PROJECT_DIR/iac/envoy/envoy.yaml"
  [[ -f "$envoy_config" ]] || { error "Envoy config not found: $envoy_config"; return 1; }

  inf "Validating envoy configuration..."
  if envoy --mode validate -c "$envoy_config" >/dev/null 2>&1; then
    ui.closer.ok:
  else
    ui.closer.not-ok:
    error "Envoy configuration validation failed"
    envoy --mode validate -c "$envoy_config"
    return 1
  fi

  inf "Starting envoy..."
  if cluster.is-running "[e]nvoy"; then
    ui.closer.kind-of-ok:
    success "envoy is already running."
    return 0
  fi

  nohup envoy -c "$envoy_config" --log-path /var/log/envoy.log >/dev/null 2>&1 &
  sleep 2

  if cluster.is-running "[e]nvoy"; then
    ui.closer.ok:
    success "envoy started."
  else
    ui.closer.not-ok:
    error "envoy failed to start."
    return 1
  fi
}

envoy.stop() { component.stop "envoy" "[e]nvoy"; }
envoy.restart() { envoy.stop; sleep 1; envoy.start; }
envoy.status() {
  component.status "envoy" "[e]nvoy"
  if cluster.is-running "[e]nvoy" && command -v curl >/dev/null 2>&1; then
    echo "  Admin: http://127.0.0.1:9901"
    if curl -s http://127.0.0.1:9901/ready >/dev/null 2>&1; then
      echo "  Cluster status:"
      curl -s http://127.0.0.1:9901/clusters 2>/dev/null | grep -E "^gomoku_cluster::" | head -5
    fi
  fi
}

# =============================================================================
# FRONTEND (Vite)
# =============================================================================

frontend.start() {
  inf "Starting frontend dev server..."

  if cluster.is-running "[v]ite"; then
    ui.closer.kind-of-ok:
    success "frontend dev server is already running."
    return 0
  fi

  local frontend_dir="$PROJECT_DIR/frontend"
  [[ -f "$frontend_dir/package.json" ]] || { error "frontend/package.json not found."; return 1; }

  if [[ ! -d "$frontend_dir/node_modules" ]]; then
    inf "Installing frontend dependencies..."
    (cd "$frontend_dir" && npm install) >/dev/null 2>&1
    ui.closer.ok:
  fi

  nohup bash -c "cd '$frontend_dir' && VITE_API_BASE=http://127.0.0.1:10000 npm run dev" >/var/log/gomoku-frontend.log 2>&1 &
  sleep 2

  if cluster.is-running "[v]ite"; then
    ui.closer.ok:
    success "frontend dev server started (http://localhost:5173)."
    open "http://localhost:5173" 2>/dev/null || true
  else
    ui.closer.not-ok:
    error "frontend dev server failed to start. Check /var/log/gomoku-frontend.log"
    return 1
  fi
}

frontend.stop() { component.stop "frontend" "[v]ite"; }
frontend.restart() { frontend.stop; sleep 1; frontend.start; }
frontend.status() {
  component.status "frontend" "[v]ite"
  cluster.is-running "[v]ite" && echo "  URL: http://localhost:5173"
}

# =============================================================================
# GOMOKU WORKERS
# =============================================================================

gomoku.start() {
  local workers="${1:-${DEFAULT_WORKERS}}"
  inf "Starting gomoku-httpd (${workers} workers)..."

  if cluster.is-running "[g]omoku-httpd" "gomoku-http-client"; then
    info "Stopping existing gomoku-httpd instances first..."
    "$SCRIPT_DIR/gomoku-httpd-ctl" stop
    sleep 1
  fi

  if cluster.is-running "[g]omoku-httpd" "gomoku-http-client"; then
    ui.closer.not-ok:
    error "Could not stop existing gomoku-httpd instances."
    return 1
  fi

  "$SCRIPT_DIR/gomoku-httpd-ctl" start -w "$workers"
  sleep 1

  if cluster.is-running "[g]omoku-httpd" "gomoku-http-client"; then
    ui.closer.ok:
    success "gomoku-httpd started (${workers} workers)."
  else
    ui.closer.not-ok:
    error "gomoku-httpd failed to start."
    return 1
  fi
}

gomoku.stop() {
  inf "Stopping gomoku-httpd..."

  if ! cluster.is-running "[g]omoku-httpd" "gomoku-http-client"; then
    ui.closer.kind-of-ok:
    success "gomoku-httpd is not running."
    return 0
  fi

  "$SCRIPT_DIR/gomoku-httpd-ctl" stop
  sleep 2

  # Force kill any survivors
  if cluster.is-running "[g]omoku-httpd" "gomoku-http-client"; then
    cluster.kill "[g]omoku-httpd" "gomoku-http-client"
    sleep 1
  fi

  if cluster.is-running "[g]omoku-httpd" "gomoku-http-client"; then
    ui.closer.not-ok:
    error "Failed to stop gomoku-httpd."
    return 1
  fi
  ui.closer.ok:
  success "gomoku-httpd stopped."
}

gomoku.restart() { gomoku.stop; sleep 1; gomoku.start; }
gomoku.status() {
  component.status "gomoku-httpd" "[g]omoku-httpd" "gomoku-http-client"
  if cluster.is-running "[g]omoku-httpd" "gomoku-http-client"; then
    local -a pids
    mapfile -t pids < <(cluster.pids "[g]omoku-httpd" "gomoku-http-client")
    local count=${#pids[@]}
    echo "  Checking ${count} instances:"
    local port
    for ((i = 0; i < count; i++)); do
      port=$((9500 + i))
      if curl -s --max-time 1 "http://127.0.0.1:$port/ready" >/dev/null 2>&1; then
        echo "    :$port responding"
      else
        echo "    :$port not responding"
      fi
    done
  fi
}

# =============================================================================
# Command dispatch — eliminates repetition in stop/restart/status
# =============================================================================

# Route <action> to the right component function.
component.dispatch() {
  local action="$1" component="$2"
  shift 2
  case "$component" in
    nginx)              "nginx.${action}" "$@" ;;
    haproxy)            "haproxy.${action}" "$@" ;;
    envoy)              "envoy.${action}" "$@" ;;
    gomoku | gomoku-httpd) "gomoku.${action}" "$@" ;;
    frontend)           "frontend.${action}" "$@" ;;
    *) error "Unknown component: $component"; return 1 ;;
  esac
}

# Parse -c flags and dispatch an action to each component.
# Usage: cmd_for_each <action> <default-order-array-name> "$@"
cmd_for_each() {
  local action="$1"
  shift
  local -n default_order=$1
  shift

  local -a components=()
  while [[ $# -gt 0 ]]; do
    case "$1" in
      -c | --component) components+=("$2"); shift 2 ;;
      *) error "Unknown option: $1"; cmd_help; return 1 ;;
    esac
  done
  [[ ${#components[@]} -eq 0 ]] && components=("${default_order[@]}")

  for c in "${components[@]}"; do
    component.dispatch "$action" "$c"
    [[ "$action" == "status" ]] && echo
  done
}

# =============================================================================
# Top-level commands
# =============================================================================

cmd_start() {
  local proxy=${DEFAULT_PROXY}
  local -a components=()
  local workers=

  while [[ $# -gt 0 ]]; do
    case $1 in
      -w | --workers) workers="$2"; shift 2
        [[ ${workers} =~ ^[0-9]+$ ]] || { error "Invalid workers: $workers"; return 1; }
        ;;
      -p | --proxy) proxy="$2"; shift 2 ;;
      -c | --component) components+=("$2"); shift 2 ;;
      *) shift ;;
    esac
  done

  [[ -n "$proxy" && "$proxy" != "haproxy" && "$proxy" != "envoy" ]] && {
    error "Invalid proxy: $proxy. Must be 'haproxy' or 'envoy'"
    return 1
  }
  [[ -z "$workers" ]] && workers=${DEFAULT_WORKERS}

  if [[ ${#components[@]} -eq 0 ]]; then
    generate.configs "$workers" "$proxy"
    nginx.start
    if [[ "$proxy" == "haproxy" ]]; then haproxy.start; else envoy.start; fi
    gomoku.start "$workers"
    frontend.start

    if [[ "$proxy" == "haproxy" ]]; then
      local url
      url=$(grep -A 3 stats "${BREW_HOME}/etc/haproxy.cfg" | grep bind | awk '{print $2}' | head -1)
      if [[ -n "$url" ]]; then open "http://${url}/stats" 2>/dev/null || true; fi
    else
      open "http://127.0.0.1:9901" 2>/dev/null || true
    fi
  else
    for c in "${components[@]}"; do
      component.dispatch start "$c" "$workers"
    done
  fi

  success "Cluster started."
  sleep 1
  cmd_ps
}

cmd_stop()    { cmd_for_each stop    STOP_ORDER  "$@"; }
cmd_restart() { cmd_for_each restart STOP_ORDER  "$@"; }
cmd_status()  { cmd_for_each status  STOP_ORDER  "$@"; }

cmd_ps() {
  export GREP_COLOR='1;33'
  local output
  output=$(/bin/ps -eo pid,ppid,%cpu,rss,command |
    /usr/bin/grep -E '[g]omoku|[h]aproxy|[e]nvoy|[n]ginx|[v]ite|PID' |
    /usr/bin/grep -Ev 'gctl|grep')

  if [[ -n "$output" ]]; then
    arrow.blk-on-ylw "Gomoku Cluster processes (sorted by PID):"
    echo
    echo "$output"
    echo
    hr
  else
    error "No Gomoku Cluster processes found."
    return 1
  fi
}

cmd_observe() {
  local tool="${1:-btop}"
  case "$tool" in
    htop) htop -F "$HTOP_REGEX" ;;
    btop) btop --filter gomoku ;;
    btm | bottom) btm ;;
    ctop) ctop ;;
    *) error "Unknown tool: $tool. Use: htop, btop, ctop, btm"; return 1 ;;
  esac
}

# =============================================================================
# Details & Help
# =============================================================================

cmd_details() {
  echo -e "
${bldblu}DETAILS:${clr}
  This script is a ${bold}controller for the entire distributed Gomoku Cluster${clr},
  which can start, stop, or examine the entire set of components that the cluster
  comprises. It can also start/stop individual components of the cluster.

  On MacOS we replace default nginx.conf and haproxy.cfg with symlinks into this
  repo's folder: ./iac/config/nginx.conf and ./iac/config/haproxy.cfg
  respectively during ${bldylw}gctl setup${clr}.

  ${bldcyn}The components:${clr}

  1. ${bldgrn}nginx${clr} (requires sudo to start/stop, because we bind to port 80 and 443).
     Nginx configuration sets up ${bldylw}dev.gomoku.games${clr} as the front, listening on
     ports 80 and 443 for that domain. The DNS points to 127.0.0.1.

  2. ${bldgrn}Vite dev server${clr} provides the React frontend for development.

  3. ${bldgrn}A reverse proxy${clr} (default: envoy, but haproxy supported) routes traffic
     to a backend of gomoku-httpd worker processes (defaults to CPU core count).

  4. ${bldgrn}gomoku-httpd${clr} workers, configurable via the ${bldylw}-w${clr} flag. Each worker
     listens on its own port, offering a secondary port for haproxy and
     endpoints /health and /ready for envoy. Since gomoku-httpd is single-threaded,
     each worker responds to one move at a time — proxies are signaled if busy.
     The ${bldylw}-w${clr} flag also regenerates proxy configs from iac/templates/.
"
}

cmd_help() {
  local binary
  binary="$(basename "$0")"
  local c="${txtpur}${italic}"
  echo -e "
${bldblu}USAGE:${clr}
  ${bldylw}${binary} <command> [options]${clr}

${bldblu}DESCRIPTION:${clr}
  ${bldylw}WARNING: ${txtred}This script manages the Gomoku Cluster in development mode.
  It is not intended for production use.${clr}

  Proxy configs are auto-generated from templates in ${bldylw}iac/templates/${clr}
  to match the number of workers (${bldylw}-w${clr} flag).

${bldblu}COMMANDS:${clr}
  ${c}# Boot the entire cluster. Envoy is the default proxy.${clr}
  ${bldylw}${binary} start [ -p <haproxy|envoy> ] [ -w <workers> ] [ -c <component> ... ]${clr}

  ${c}# Stop / restart / query status of the cluster or individual components.${clr}
  ${bldylw}${binary} stop    [ -c <component> ... ]${clr}
  ${bldylw}${binary} restart [ -c <component> ... ]${clr}
  ${bldylw}${binary} status  [ -c <component> ... ]${clr}

  ${c}# Show cluster processes sorted by CPU usage.${clr}
  ${bldylw}${binary} ps${clr}

  ${c}# Launch a monitoring tool filtered to Gomoku processes.${clr}
  ${bldylw}${binary} observe [ htop | btop | ctop | btm ]${clr}

  ${c}# First-time setup: create log files, install utilities.${clr}
  ${bldylw}${binary} setup${clr}

  ${c}# Show more details about this script.${clr}
  ${bldylw}${binary} details${clr}

${bldblu}COMPONENTS:${clr}
  ${bldylw}nginx${clr}      Nginx web server (ports 80, 443)
  ${bldylw}haproxy${clr}    HAProxy load balancer (frontend :10000, admin :8404)
  ${bldylw}envoy${clr}      Envoy proxy (frontend :10000, admin :9901)
  ${bldylw}gomoku${clr}     Gomoku-httpd worker cluster (ports 9500+)
  ${bldylw}frontend${clr}   Vite React dev server (http://localhost:5173)

${bldblu}START OPTIONS:${clr}
  ${bldylw}-p, --proxy <haproxy|envoy>${clr}    Reverse proxy (default: envoy)
  ${bldylw}-w, --workers <N>${clr}              Number of workers (default: ${DEFAULT_WORKERS})
  ${bldylw}-c, --component <name>${clr}         Start a specific component (repeatable)

${bldblu}EXAMPLES:${clr}
  ${bldylw}${binary} start${clr}                          # Envoy, ${DEFAULT_WORKERS} workers
  ${bldylw}${binary} start -w 16${clr}                    # Envoy, 16 workers
  ${bldylw}${binary} start -p haproxy -w 8${clr}          # HAProxy, 8 workers
  ${bldylw}${binary} start -c nginx -c gomoku${clr}       # Start only nginx and gomoku
  ${bldylw}${binary} stop${clr}                           # Stop entire cluster
  ${bldylw}${binary} stop -c envoy${clr}                  # Stop only envoy
  ${bldylw}${binary} status${clr}                         # Show all component status
  ${bldylw}${binary} ps${clr}                             # Process table
  ${bldylw}${binary} observe btop${clr}                   # Launch btop
  ${bldylw}${binary} setup${clr}                          # First-time setup
"
}

# =============================================================================
# Main / argument parsing
# =============================================================================

parse-args() {
  case "${1:-}" in
    start)   shift; cmd_start "$@" ;;
    stop)    shift; cmd_stop "$@" ;;
    restart) shift; cmd_restart "$@" ;;
    status)  shift; cmd_status "$@" ;;
    setup)   shift; cmd_setup "$@" ;;
    observe) shift; cmd_observe "$@" ;;
    ps)      shift; cmd_ps "$@" ;;
    details) shift; cmd_details "$@" ;;
    "" | -h | --help | help) cmd_help ;;
    *) error "Unknown command: $1"; cmd_help; return 1 ;;
  esac
}

declare -a ARGV=("$@")
export ARGV

setup.bashmatic

# Override bashmatic functions with preferred wrappers
warning() { arrow.blk-on-ylw "$@"; }
hint() { panel-info-blue-black "$@"; }

setup.direnv

[[ "${ARGV[*]}" =~ setup ]] || setup.log-files-are-ok || exit 1

if [[ ${#ARGV[@]} -eq 0 ]]; then
  cmd_help
  exit 0
fi

parse-args "${ARGV[@]}"
