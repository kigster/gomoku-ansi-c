#!/usr/bin/env bash
# shellcheck disable=SC2126

set +e

# =============================================================================
# Globals
# =============================================================================
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd -P)"
export SCRIPT_DIR
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
export PROJECT_DIR

export PROJECT_ROOT="$(pwd -P)"

[[ -s "${PROJECT_DIR}/Makefile" ]] || {
  error "PROJECT_DIR is not valid." "PROJECT_DIR: ${PROJECT_DIR}"
  return 1
}

OS="$(uname -s | tr '[:upper:]' '[:lower:]')"
export OS
export DEFAULT_PROXY="envoy"
export NGINX_USER="_www"

declare -i DEFAULT_WORKERS=10
if [[ ${OS} == darwin ]]; then
  DEFAULT_WORKERS=$(sysctl -n hw.physicalcpu)
elif [[ ${OS} == linux ]]; then
  DEFAULT_WORKERS=$(nproc)
fi
export DEFAULT_WORKERS

if [[ $OS == darwin ]]; then
  BREW_HOME=$(brew --prefix)
fi

declare -a LOG_FILES=(
  "/var/log/nginx/access.log"
  "/var/log/nginx/error.log"
  "/var/log/envoy.log"
  "/var/log/haproxy.log"
  "/var/log/gomoku-httpd.log"
  "/var/log/gomoku-frontend.log"
)
export LOG_FILES

declare -a PACKAGES=(clang-format shfmt btop htop ctop bottom)
export PACKAGES

export HTOP_REGEX='|nginx|haproxy|envoy|gomoku-httpd|vite'

# Component order for stop/restart/status (reverse of start)
declare -a STOP_ORDER=(frontend gomoku envoy haproxy nginx)
export STOP_ORDER
export START_COMPONENTS=(nginx haproxy envoy gomoku frontend)

# =============================================================================
# Process Management — self-contained, no bashmatic dependency
# =============================================================================

# Return PIDs matching $1, excluding lines matching $2.
# Uses bracket trick: pass "[n]ginx" so grep never matches itself.
# Always excludes monitoring tools (htop, btop, ctop, btm, gctl).
# Usage: cluster.pids "[e]nvoy"
#        cluster.pids "[g]omoku-httpd" "gomoku-http-client"
cluster.pids() {
  local pattern="$1"
  local exclude="${2:-}"
  local base_exclude="htop|btop|ctop|btm|gctl"
  if [[ -n "$exclude" ]]; then
    base_exclude="${base_exclude}|${exclude}"
  fi
  /bin/ps -eo pid,command |
    /usr/bin/grep -E "$pattern" |
    /usr/bin/grep -Ev "$base_exclude" |
    /usr/bin/awk '{print $1}' |
    /usr/bin/sort -n
}

# Return 0 if any PIDs match, 1 otherwise.
cluster.is-running() {
  [[ -n "$(cluster.pids "$@")" ]]
}

# Send TERM, wait, then KILL survivors. Optionally sudo.
cluster.kill() {
  local pattern="$1"
  local exclude="${2:-}"
  local use_sudo="${3:-}"
  local -a pids
  local pid

  mapfile -t pids < <(cluster.pids "$pattern" "$exclude")
  [[ ${#pids[@]} -eq 0 ]] && return 0

  for pid in "${pids[@]}"; do
    if [[ -n "$pid" ]]; then
      ${use_sudo} /bin/kill -TERM "$pid" 2>/dev/null || true
    fi
  done
  sleep 1

  mapfile -t pids < <(cluster.pids "$pattern" "$exclude")
  for pid in "${pids[@]}"; do
    if [[ -n "$pid" ]]; then
      ${use_sudo} /bin/kill -9 "$pid" 2>/dev/null || true
    fi
  done
}

# =============================================================================
# Setup
# =============================================================================

setup.bashmatic() {
  [[ -d ~/.bashmatic ]] || bash -c "$(curl -fsSL https://bashmatic.re1.re); bashmatic-install -q" >/dev/null 2>&1
  # shellcheck disable=SC1090
  source ~/.bashmatic/init >/dev/null 2>&1
}

setup.log-files-are-ok() {
  for file in "${LOG_FILES[@]}"; do
    [[ $file =~ nginx ]] && continue
    [[ -w "$file" ]] || {
      error "Log file [${file}] either does not exist or is not writable."
      sleep 1
      hint "HINT: Run the following command first:" "$(basename "$0") setup"
      return 1
    }
  done
}

setup.sudo() {
  h3 "Setting up sudo and the ownership of the log files..."
  sudo -n date >/dev/null 2>&1 || {
    h.yellow "sudo is not authenticated, please enter your password to authenticate:"
    sudo -v || {
      error "sudo authentication failed."
      return 1
    }
  }

  run "sudo chmod 777 /var/log"
  for file in "${LOG_FILES[@]}"; do
    run "sudo mkdir -p $(dirname "$file")"
    run "sudo touch $file"
    run "sudo chown $USER $file"
    run "sudo chmod 666 $file"
  done
  run "sudo chown -R $NGINX_USER /var/log/nginx"
  success "All log files exist and are writeable."
}

setup.direnv() {
  command -v direnv >/dev/null 2>&1 || package.install direnv
  command -v direnv >/dev/null 2>&1 && direnv allow .
}

setup.utils() {
  for pkg in "${PACKAGES[@]}"; do
    inf "Checking if $pkg is installed..."
    if package.is-installed "$pkg"; then
      ui.closer.ok:
    else
      ui.closer.kind-of-ok:
      inf "Installing $pkg..."
      package.install "$pkg" >/dev/null 2>&1 || true
      ui.closer.ok:
    fi
  done
}

cmd_setup() {
  setup.bashmatic
  setup.sudo
  setup.direnv
  setup.utils
}

cmd_certificates() {
  inf "Generating certificates..."
  local domain_config="${HOME}/.dnsmadeeasy/credentials.ini"
  local certificates_dir="${HOME}/.letsencrypt"
  [[ -s "${domain_config}" ]] || {
    error "DNS Made Easy API keys were not found." "Expected to find them in ${domain_config}."
    return 1
  }
  command -v certbot >/dev/null 2>&1 || {
    package.install certbot
    command -v certbot >/dev/null 2>&1 || {
      error "certbot is not installed."
      return 1
    }
  }
  certbot certonly \
    --dns-dnsmadeeasy \
    --dns-dnsmadeeasy-credentials "${HOME}/.dnsmadeeasy/credentials.ini" \
    -d dev.gomoku.games \
    --dns-dnsmadeeasy-propagation-seconds 120 \
    --work-dir "${certificates_dir}" \
    --config-dir "${certificates_dir}" \
    --logs-dir "${certificates_dir}/log"
  success "certificates generated."
  ui.closer.ok:
}

# =============================================================================
# Config Generation
# =============================================================================

cmd_configs() {
  local workers="${1:-10}"
  local template_dir="$PROJECT_DIR/iac/templates"
  local output

  local nginx_config
  nginx_config="$(nginx -h 2>&1 | grep -- '-c.*default' | tr -d ')' | awk 'BEGIN{FS="default: "}{print $2}')"

  local nginx_home
  nginx_home="$(dirname "${nginx_config}")"

  h3 "Generating haproxy.cfg config for ${workers} workers..."
  output="${BREW_HOME}/etc/haproxy.cfg"
  bash "$template_dir/haproxy.cfg.sh" "$workers" >"$output"
  success "haproxy.cfg config generated."

  h3 "Generating envoy.yaml config for ${workers} workers..."
  output="$PROJECT_DIR/iac/envoy/envoy.yaml"
  bash "$template_dir/envoy.yaml.sh" "$workers" >"$output"
  success "envoy.yaml config generated."

  h3 "Generating nginx.conf..."
  output="$PROJECT_DIR/iac/config/nginx.conf"
  bash "$template_dir/nginx.conf.sh" >"$output"
  success "nginx.conf config generated."

  if [[ -f ${nginx_home}/nginx.conf ]]; then
    run "sudo mv ${nginx_home}/nginx.conf ${nginx_home}/nginx.conf.bak"
    run "sudo ln -nfs ${PROJECT_DIR}/iac/config/nginx.conf ${nginx_home}/nginx.conf"
    success "${nginx_home}/nginx.conf has been symlinked."
  elif [[ -d ${nginx_home} ]]; then
    run "sudo ln -nfs ${PROJECT_DIR}/iac/config/nginx.conf ${nginx_home}/nginx.conf"
    success "${nginx_home}/nginx.conf has been symlinked."
  else
    error "Couldn't determine nginx home directory. " \
      "Run 'nginx -h' to see the default nginx.conf file location," \
      "and ensure that the enclosing directory exists."
    return 1
  fi
}

# =============================================================================
# Generic component stop / status — covers haproxy, envoy, frontend
# =============================================================================

# Generic stop: kill processes matching a grep pattern.
# Usage: component.stop <display-name> <grep-pattern> [exclude] [sudo]
component.stop() {
  local name="$1" pattern="$2" exclude="${3:-}" use_sudo="${4:-}"
  inf "Stopping $name..."

  if cluster.is-running "$pattern" "$exclude"; then
    cluster.kill "$pattern" "$exclude" "$use_sudo"
    sleep 1
    if cluster.is-running "$pattern" "$exclude"; then
      ui.closer.not-ok:
      error "$name failed to stop."
      return 1
    fi
    ui.closer.ok:
    success "$name stopped."
  else
    ui.closer.kind-of-ok:
    success "$name is not running."
  fi
}

# Generic status: show PIDs for a grep pattern.
component.status() {
  local name="$1" pattern="$2" exclude="${3:-}"
  inf "$name status..."

  if cluster.is-running "$pattern" "$exclude"; then
    ui.closer.ok:
    success "$name is running (PIDs: $(cluster.pids "$pattern" "$exclude" | tr '\n' ' '))"
  else
    ui.closer.kind-of-ok:
    warning "$name is not running."
  fi
}

# =============================================================================
# NGINX — requires sudo
# =============================================================================

nginx.start() {
  inf "Starting nginx..."

  if cluster.is-running "[n]ginx"; then
    ui.closer.kind-of-ok:
    success "nginx already running, $(cluster.pids '[n]ginx' | wc -l | tr -d ' ') processes."
    return 0
  fi

  info "Testing nginx configuration..."
  run.set-next abort-on-error
  run "sudo ${BREW_HOME}/bin/nginx -t"
  run "sudo ${BREW_HOME}/bin/nginx -g \"daemon on;\""
  sleep 2

  if cluster.is-running "[n]ginx"; then
    ui.closer.ok:
    success "nginx started."
    (sleep 2 && open "https://dev.gomoku.games" 2>/dev/null || true) &
    return 0
  else
    ui.closer.not-ok:
    error "nginx failed to start — check /var/log/nginx/error.log"
    return 1
  fi
}

nginx.stop() {
  inf "Stopping nginx..."

  if ! cluster.is-running "[n]ginx"; then
    ui.closer.kind-of-ok:
    success "nginx is not running."
    return 0
  fi

  sudo "${BREW_HOME}/bin/nginx" -s stop 2>/dev/null || true
  sleep 2

  # Force-kill any survivors via xargs (handles root-owned processes)
  if cluster.is-running "[n]ginx"; then
    warning "nginx didn't stop gracefully, force killing..."
    cluster.pids "[n]ginx" | sudo xargs /bin/kill -9 2>/dev/null || true
    sleep 1
  fi

  if cluster.is-running "[n]ginx"; then
    ui.closer.not-ok:
    error "nginx failed to stop."
    return 1
  fi
  ui.closer.ok:
  success "nginx stopped."
}

nginx.restart() {
  nginx.stop
  sleep 1
  nginx.start
}
nginx.status() {
  component.status "nginx" "[n]ginx"
  if cluster.is-running "[n]ginx" && command -v curl >/dev/null 2>&1; then
    curl -s http://127.0.0.1/nginx_status 2>/dev/null || true
  fi
}

# =============================================================================
# HAPROXY
# =============================================================================

haproxy.start() {
  inf "Starting haproxy..."

  if cluster.is-running "[h]aproxy"; then
    ui.closer.kind-of-ok:
    success "haproxy is already running."
    return 0
  fi

  "${BREW_HOME}/opt/haproxy/bin/haproxy" -D -f "${BREW_HOME}/etc/haproxy.cfg"
  sleep 1

  if cluster.is-running "[h]aproxy"; then
    ui.closer.ok:
    success "haproxy started."
  else
    ui.closer.not-ok:
    error "haproxy failed to start."
    return 1
  fi
}

haproxy.stop() { component.stop "haproxy" "[h]aproxy"; }
haproxy.restart() {
  haproxy.stop
  sleep 1
  haproxy.start
}
haproxy.status() {
  component.status "haproxy" "[h]aproxy"
  if cluster.is-running "[h]aproxy" && [[ -f "${BREW_HOME}/etc/haproxy.cfg" ]]; then
    local url
    url=$(grep -A 3 stats "${BREW_HOME}/etc/haproxy.cfg" | grep bind | awk '{print $2}' | head -1)
    [[ -n "$url" ]] && echo "  Admin: http://${url}/stats"
  fi
}

# =============================================================================
# ENVOY
# =============================================================================

envoy.start() {
  inf "Checking if envoy is installed..."
  if ! command -v envoy >/dev/null 2>&1; then
    ui.closer.not-ok:
    inf "Installing envoy..."
    package.install envoy
    command -v envoy >/dev/null 2>&1 || {
      error "envoy is not installed."
      return 1
    }
    ui.closer.ok:
  else
    ui.closer.ok:
  fi

  local envoy_config="$PROJECT_DIR/iac/envoy/envoy.yaml"
  [[ -f "$envoy_config" ]] || {
    error "Envoy config not found: $envoy_config"
    return 1
  }

  inf "Validating envoy configuration..."
  if envoy --mode validate -c "$envoy_config" >/dev/null 2>&1; then
    ui.closer.ok:
  else
    ui.closer.not-ok:
    error "Envoy configuration validation failed"
    envoy --mode validate -c "$envoy_config"
    return 1
  fi

  inf "Starting envoy..."
  if cluster.is-running "[e]nvoy"; then
    ui.closer.kind-of-ok:
    success "envoy is already running."
    return 0
  fi

  nohup envoy -c "$envoy_config" --log-path /var/log/envoy.log >/dev/null 2>&1 &
  sleep 2

  if cluster.is-running "[e]nvoy"; then
    ui.closer.ok:
    success "envoy started."
  else
    ui.closer.not-ok:
    error "envoy failed to start."
    return 1
  fi
}

envoy.stop() { component.stop "envoy" "[e]nvoy"; }
envoy.restart() {
  envoy.stop
  sleep 1
  envoy.start
}
envoy.status() {
  component.status "envoy" "[e]nvoy"
  if cluster.is-running "[e]nvoy" && command -v curl >/dev/null 2>&1; then
    echo "  Admin: http://127.0.0.1:9901"
    if curl -s http://127.0.0.1:9901/ready >/dev/null 2>&1; then
      echo "  Cluster status:"
      curl -s http://127.0.0.1:9901/clusters 2>/dev/null | grep -E "^gomoku_cluster::" | head -5
    fi
  fi
}

# =============================================================================
# FRONTEND (Vite)
# =============================================================================

frontend.build() {
  inf "Building frontend assets..."
  local frontend_dir="$PROJECT_DIR/frontend"
  [[ -d "$frontend_dir/dist" ]] || {
    error "frontend/dist directory not found."
    return 1
  }
  (cd "$frontend_dir" && npm run build) >/dev/null 2>&1
  ui.closer.ok:
  sudo find "${PROJECT_DIR}/frontend/dist/" -type d -exec chmod 711 {} +
  sudo chmod -R 755 "${PROJECT_DIR}/frontend/dist/"
  success "frontend assets built."
}

frontend.start() {
  inf "Starting frontend dev server..."

  # This is for nginx to serve static assets
  frontend.build

  if cluster.is-running "[v]ite"; then
    ui.closer.kind-of-ok:
    success "frontend dev server is already running."
    return 0
  fi

  local frontend_dir="$PROJECT_DIR/frontend"
  [[ -f "$frontend_dir/package.json" ]] || {
    error "frontend/package.json not found."
    return 1
  }

  if [[ ! -d "$frontend_dir/node_modules" ]]; then
    inf "Installing frontend dependencies..."
    (cd "$frontend_dir" && npm install) >/dev/null 2>&1
    ui.closer.ok:
  fi

  nohup bash -c "cd '$frontend_dir' && VITE_API_BASE=http://127.0.0.1:10000 npm run dev" >/var/log/gomoku-frontend.log 2>&1 &
  sleep 2

  if cluster.is-running "[v]ite"; then
    ui.closer.ok:
    success "frontend dev server started (http://localhost:5173)."
    open "http://localhost:5173" 2>/dev/null || true
  else
    ui.closer.not-ok:
    error "frontend dev server failed to start. Check /var/log/gomoku-frontend.log"
    return 1
  fi
}

frontend.stop() { component.stop "frontend" "[v]ite"; }
frontend.restart() {
  frontend.stop
  sleep 1
  frontend.start
}
frontend.status() {
  component.status "frontend" "[v]ite"
  cluster.is-running "[v]ite" && echo "  URL: http://localhost:5173"
}

# =============================================================================
# GOMOKU WORKERS
# =============================================================================

gomoku.start() {
  local workers="${1:-${DEFAULT_WORKERS}}"
  inf "Starting gomoku-httpd (${workers} workers)..."

  if cluster.is-running "[g]omoku-httpd" "gomoku-http-client"; then
    info "Stopping existing gomoku-httpd instances first..."
    "$SCRIPT_DIR/gomoku-httpd-ctl" stop
    sleep 1
  fi

  if cluster.is-running "[g]omoku-httpd" "gomoku-http-client"; then
    ui.closer.not-ok:
    error "Could not stop existing gomoku-httpd instances."
    return 1
  fi

  "$SCRIPT_DIR/gomoku-httpd-ctl" start -w "$workers"
  sleep 1

  if cluster.is-running "[g]omoku-httpd" "gomoku-http-client"; then
    ui.closer.ok:
    success "gomoku-httpd started (${workers} workers)."
  else
    ui.closer.not-ok:
    error "gomoku-httpd failed to start."
    return 1
  fi
}

gomoku.stop() {
  inf "Stopping gomoku-httpd..."

  if ! cluster.is-running "[g]omoku-httpd" "gomoku-http-client"; then
    ui.closer.kind-of-ok:
    success "gomoku-httpd is not running."
    return 0
  fi

  "$SCRIPT_DIR/gomoku-httpd-ctl" stop
  sleep 2

  # Force kill any survivors
  if cluster.is-running "[g]omoku-httpd" "gomoku-http-client"; then
    cluster.kill "[g]omoku-httpd" "gomoku-http-client"
    sleep 1
  fi

  if cluster.is-running "[g]omoku-httpd" "gomoku-http-client"; then
    ui.closer.not-ok:
    error "Failed to stop gomoku-httpd."
    return 1
  fi
  ui.closer.ok:
  success "gomoku-httpd stopped."
}

gomoku.restart() {
  gomoku.stop
  sleep 1
  gomoku.start
}
gomoku.status() {
  component.status "gomoku-httpd" "[g]omoku-httpd" "gomoku-http-client"
  if cluster.is-running "[g]omoku-httpd" "gomoku-http-client"; then
    local -a pids
    mapfile -t pids < <(cluster.pids "[g]omoku-httpd" "gomoku-http-client")
    local count=${#pids[@]}
    echo "  Checking ${count} instances:"
    local port
    for ((i = 0; i < count; i++)); do
      port=$((9500 + i))
      if curl -s --max-time 1 "http://127.0.0.1:$port/ready" >/dev/null 2>&1; then
        echo "    :$port responding"
      else
        echo "    :$port not responding"
      fi
    done
  fi
}

# =============================================================================
# Command dispatch — eliminates repetition in stop/restart/status
# =============================================================================

# Route <action> to the right component function.
component.dispatch() {
  local action="$1" component="$2"
  shift 2
  case "$component" in
  nginx) "nginx.${action}" "$@" ;;
  haproxy) "haproxy.${action}" "$@" ;;
  envoy) "envoy.${action}" "$@" ;;
  gomoku | gomoku-httpd) "gomoku.${action}" "$@" ;;
  frontend) "frontend.${action}" "$@" ;;
  *)
    error "Unknown component: $component"
    return 1
    ;;
  esac
}

# Parse -c flags and dispatch an action to each component.
# Usage: cmd_for_each <action> <default-order-array-name> "$@"
cmd_for_each() {
  local action="$1"
  shift
  local -n default_order=$1
  shift

  local -a components=()
  while [[ $# -gt 0 ]]; do
    case "$1" in
    *)
      if array.includes "$1" "${START_COMPONENTS[@]}"; then
        components+=("$1")
        shift 1
      else
        error "Unknown component or directive: $1"
        return 1
      fi
      ;;
    esac
  done
  [[ ${#components[@]} -eq 0 ]] && components=("${default_order[@]}")

  for c in "${components[@]}"; do
    component.dispatch "$action" "$c"
    [[ "$action" == "status" ]] && echo
  done
}

# =============================================================================
# Top-level commands
# =============================================================================

cmd_start() {
  local proxy=${DEFAULT_PROXY}
  local -a components=()
  local workers=

  run.set-all abort-on-error

  while [[ $# -gt 0 ]]; do
    case $1 in
    -w | --workers)
      workers="$2"
      shift 2
      [[ ${workers} =~ ^[0-9]+$ ]] || {
        error "Invalid workers: $workers"
        return 1
      }
      ;;
    -p | --proxy)
      proxy="$2"
      shift 2
      ;;

    *)
      if array.includes "$1" "${START_COMPONENTS[@]}"; then
        components+=("$1")
        shift 1
      else
        error "Unknown component or directive: $1"
        return 1
      fi
      ;;
    esac
  done

  [[ -n "$proxy" && "$proxy" != "haproxy" && "$proxy" != "envoy" ]] && {
    error "Invalid proxy: $proxy. Must be 'haproxy' or 'envoy'"
    return 1
  }

  [[ -z "$workers" ]] && workers=${DEFAULT_WORKERS}
  cmd_configs "$workers"

  if [[ ${#components[@]} -eq 0 ]]; then
    nginx.start
    if [[ "$proxy" == "haproxy" ]]; then haproxy.start; else envoy.start; fi
    gomoku.start "$workers"
    frontend.start

    if [[ "$proxy" == "haproxy" ]]; then
      local url
      url=$(grep -A 3 stats "${BREW_HOME}/etc/haproxy.cfg" | grep bind | awk '{print $2}' | head -1)
      if [[ -n "$url" ]]; then open "http://${url}/stats" 2>/dev/null || true; fi
    else
      open "http://127.0.0.1:9901" 2>/dev/null || true
    fi
  else
    for c in "${components[@]}"; do
      component.dispatch start "$c" "$workers"
    done
  fi

  success "Cluster started."
  sleep 1
  cmd_ps
}

cmd_stop() { cmd_for_each stop STOP_ORDER "$@"; }
cmd_restart() { cmd_for_each restart STOP_ORDER "$@"; }
cmd_status() { cmd_for_each status STOP_ORDER "$@"; }

cmd_ps() {
  export GREP_COLOR='1;33'
  local output
  output=$(/bin/ps -eo pid,ppid,%cpu,rss,command |
    /usr/bin/grep -E '[g]omoku|[h]aproxy|[e]nvoy|[n]ginx|[v]ite|PID' |
    /usr/bin/grep -Ev 'gctl|grep')

  if [[ -n "$output" ]]; then
    arrow.blk-on-ylw "Gomoku Cluster processes (sorted by PID):"
    echo
    echo "$output"
    echo
    hr
  else
    error "No Gomoku Cluster processes found."
    return 1
  fi
}

cmd_observe() {
  local tool="${1:-btop}"
  case "$tool" in
  htop) htop -F "$HTOP_REGEX" ;;
  btop) btop --filter gomoku ;;
  btm | bottom) btm ;;
  ctop) ctop ;;
  *)
    error "Unknown tool: $tool. Use: htop, btop, ctop, btm"
    return 1
    ;;
  esac
}

# =============================================================================
# Details & Help
# =============================================================================

cmd_details() {
  echo -e "
${bldblu}DETAILS:${clr}
  This script is a ${bold}controller for the entire distributed Gomoku Cluster${clr},
  which can start, stop, or examine the entire set of components that the cluster
  comprises. It can also start/stop individual components of the cluster.

  We use nginx and either haproxy or envoy as the reverse proxy. Therefore 
  we need to overrwrite the default nginx.conf and haproxy.cfg with symlinks into
  this repo: sz./iac/config/nginx.conf and ./iac/config/haproxy.cfg.

  Every time you start the cluster we overwrite the default nginx.conf and haproxy.cfg.

  ${bldcyn}The components:${clr}

  1. ${bldgrn}nginx${clr} (requires sudo to start/stop, because we bind to port 80 and 443).
     Nginx configuration sets up ${bldylw}dev.gomoku.games${clr} as the front, listening on
     ports 80 and 443 for that domain. The DNS points to 127.0.0.1.

  2. ${bldgrn}Vite dev server${clr} provides the React frontend for development.

  3. ${bldgrn}A reverse proxy${clr} (default: envoy, but haproxy supported) routes traffic
     to a backend of gomoku-httpd worker processes (defaults to CPU core count).

  4. ${bldgrn}gomoku-httpd${clr} workers, configurable via the ${bldylw}-w${clr} flag. Each worker
     listens on its own port, offering a secondary port for haproxy and
     endpoints /health and /ready for envoy. Since gomoku-httpd is single-threaded,
     each worker responds to one move at a time — proxies are signaled if busy.
     The ${bldylw}-w${clr} flag also regenerates proxy configs from iac/templates/.
"
}

cmd_help() {
  local binary
  binary="$(basename "$0")"
  local c="${txtpur}${italic}"
  echo -e "
${bldblu}USAGE:${clr}
  ${bldylw}${binary} <command> [options]${clr}

${bldblu}DESCRIPTION:${clr}
  ${bldylw}WARNING: ${bldred}This script manages the Gomoku Cluster in development mode.
           It is not intended for production use.${clr}

  Proxy configs are auto-generated from templates in ${bldylw}iac/templates/${clr}
  to match the number of workers (${bldylw}-w${clr} flag), point to the proper SSL 
  certificates, and choose the appropriate event polling mechanism for 
  the OS (epoll or kqueue).

${bldblu}COMMANDS:${clr}
  ${c}# First-time setup: create log files, install utilities.${clr}
  ${bldylw}${binary} setup${clr}

  ${c}# Generate SSL certificates for dev.gomoku.games.${clr}
  ${bldylw}${binary} certificates${clr}

  ${c}# Generate haproxy, envoy and nginx configs into iac/config/ folder.${clr}
  ${bldylw}${binary} configs${clr}

  ${c}# Boot the entire cluster. Envoy is the default proxy. Components are listed below.${clr}
  ${bldylw}${binary} start [ -p <haproxy|envoy> ] [ -w <workers> ] [ <component> ... ] ${clr}

${bldblu}START OPTIONS:${clr}
  ${bldylw}-p, --proxy <haproxy|envoy>${clr}    Reverse proxy (default: envoy)
  ${bldylw}-w, --workers <N>${clr}              Number of workers (default: ${DEFAULT_WORKERS})
  ${bldylw}component${clr}                      Item(s) from the following component list:
                                 $(array.join ", " "${START_COMPONENTS[@]}")

${bldblu}OTHER COMMANDS:${clr}
  ${c}# Stop / restart / query status of the cluster or individual components.${clr}
  ${bldylw}${binary} stop    [ <component> ... ]${clr}
  ${bldylw}${binary} restart [ <component> ... ]${clr}
  ${bldylw}${binary} status  [ <component> ... ]${clr}

  ${c}# Show cluster processes sorted by CPU usage.${clr}
  ${bldylw}${binary} ps${clr}

  ${c}# Launch a monitoring tool filtered to Gomoku processes.${clr}
  ${bldylw}${binary} observe [ htop | btop | ctop | btm ]${clr}

  ${c}# Show more details about this script.${clr}
  ${bldylw}${binary} details${clr}

${bldblu}COMPONENTS:${clr}
  ${bldylw}nginx${clr}      Nginx web server (ports 80, 443)
  ${bldylw}haproxy${clr}    HAProxy load balancer (frontend :10000, admin :8404)
  ${bldylw}envoy${clr}      Envoy proxy (frontend :10000, admin :9901)
  ${bldylw}gomoku${clr}     Gomoku-httpd worker cluster (ports 9500+)
  ${bldylw}frontend${clr}   Vite React dev server (http://localhost:5173)

${bldblu}EXAMPLES:${clr}
  ${bldylw}${binary} start${clr}                          # Envoy, ${DEFAULT_WORKERS} workers
  ${bldylw}${binary} start -w 16${clr}                    # Envoy, 16 workers
  ${bldylw}${binary} start haproxy -w 8${clr}             # HAProxy, 8 workers
  ${bldylw}${binary} start gomoku nginx${clr}             # Start only nginx and gomoku
  ${bldylw}${binary} stop${clr}                           # Stop the entire cluster
  ${bldylw}${binary} stop envoy${clr}                     # Stop only envoy
  ${bldylw}${binary} status${clr}                         # Show all component status
  ${bldylw}${binary} ps${clr}                             # Process table
  ${bldylw}${binary} observe btop${clr}                   # Launch btop
  ${bldylw}${binary} setup${clr}                          # First-time setup
"
}

# =============================================================================
# Main / argument parsing
# =============================================================================

parse-args() {
  case "${1:-}" in
  start)
    shift
    cmd_start "$@"
    ;;
  stop)
    shift
    cmd_stop "$@"
    ;;
  restart)
    shift
    cmd_restart "$@"
    ;;
  status)
    shift
    cmd_status "$@"
    ;;
  setup)
    shift
    cmd_setup "$@"
    ;;
  observe)
    shift
    cmd_observe "$@"
    ;;
  ps)
    shift
    cmd_ps "$@"
    ;;
  details)
    shift
    cmd_details "$@"
    ;;
  configs)
    shift
    cmd_configs "$@"
    ;;
  certificates)
    shift
    cmd_certificates "$@"
    ;;
  "" | -h | --help | help) cmd_help ;;
  *)
    error "Unknown command: $1"
    cmd_help
    return 1
    ;;
  esac
}

declare -a ARGV=("$@")
export ARGV

setup.bashmatic

# Override bashmatic functions with preferred wrappers
warning() { arrow.blk-on-ylw "$@"; }
hint() { panel-info-blue-black "$@"; }

setup.direnv

if [[ ${#ARGV[@]} -eq 0 || "${ARGV[*]}" =~ help || "${ARGV[*]}" =~ -h || "${ARGV[*]}" =~ --help ]]; then
  cmd_help
  exit 0
fi

[[ "${ARGV[*]}" =~ setup ]] || setup.log-files-are-ok || exit 1

parse-args "${ARGV[@]}"
